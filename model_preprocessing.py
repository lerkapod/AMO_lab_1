# -*- coding: utf-8 -*-
"""model_preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S6epqa-cvr7FtBUI7aXzYzcFHsW1nBrw
"""

#Импорт необходимых модулей
import pandas as pd
import pickle #pickle - одна из стандартных библиотек Python, позволяет выполнять сериализацию и десериализацию, работает с потоком байт.
from sklearn.preprocessing import  StandardScaler
from sklearn.model_selection import train_test_split

# Загрузка тренировочных данных из train_Data_sizes.csv
data = pd.read_csv('train/train_Data_sizes.csv')

#Вывод описательной статистики по датасету
print(data.describe())
#распределение классов
count_class = data.groupby('Class').size()
print(count_class)
#Корреляционная матрица
correlations = data.corr(numeric_only = True)
print(correlations)

# Выделение зависимой (целевой) переменной y и независимой переменной-предиктора X
X = data[['bust', 'waist', 'hips']]
y = data['Class']


# Стандартизация датасета
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Разобъем датасет на обучающую и тестовую выборки
X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

#объединяем в единую структуру данных
data_preprocessing = {
    'X_train': X_train,
    'X_val': X_val,
    'y_train': y_train,
    'y_val': y_val,
    'scaler': scaler
}

# Сохраним предобработанные даннные в файл
filename = 'data_preprocessing.pkl'
with open(filename, 'wb') as f:
    pickle.dump(data_preprocessing, f)
pd.to_pickle(data_preprocessing, filename)

print(f"Данные предобработанны и успешно сохранены в файл {filename}")